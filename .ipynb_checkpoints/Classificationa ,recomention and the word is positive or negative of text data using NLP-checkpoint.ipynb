{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import nltk\n",
    "import os\n",
    "from nltk import tokenize \n",
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vivek\\\\Desktop\\\\Vivek-Upadhya.github.io'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Data\n",
    "\n",
    "raw=open(\"C:\\\\Users\\\\vivek\\\\Desktop\\\\NLP Python Practice\\\\Labeled Dateset.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and make the Data into the Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Data in lower\n",
    "\n",
    "raw=raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "\n",
    "docs=sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this recipe is very special for cooking snacks : cooking\\ni like to cook but it is usually takes longer : cokking \\nmy priorities is cooking include pastas and soup: cooking\\none need to stay fit while playing profesional sport : sports\\nit is very important for sportsman to take  care of their diet : sports\\nprofessional sports demand a lot of hardwork: sports']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into the label and review\n",
    "\n",
    "docs=docs[0].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this recipe is very special for cooking snacks : cooking',\n",
       " 'i like to cook but it is usually takes longer : cokking ',\n",
       " 'my priorities is cooking include pastas and soup: cooking',\n",
       " 'one need to stay fit while playing profesional sport : sports',\n",
       " 'it is very important for sportsman to take  care of their diet : sports',\n",
       " 'professional sports demand a lot of hardwork: sports']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation as punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs:\n",
    "    for ch in d:\n",
    "        if ch in punc:\n",
    "            d.replace(ch,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing Stop word and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "for d in docs:\n",
    "    for token in word_tokenize(d):\n",
    "        if token in ENGLISH_STOP_WORDS:\n",
    "            d.replace(token,\"\")\n",
    "            d.replace(token,ps.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Ask from the user for test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D0:this recipe is very special for cooking snacks : cooking\n",
      "D1:i like to cook but it is usually takes longer : cokking \n",
      "D2:my priorities is cooking include pastas and soup: cooking\n",
      "D3:one need to stay fit while playing profesional sport : sports\n",
      "D4:it is very important for sportsman to take  care of their diet : sports\n",
      "D5:professional sports demand a lot of hardwork: sports\n",
      "Enter your text:i like to make specual food\n",
      "Label:  ['cokking']\n",
      "Objective Statement, No openion Showed\n",
      "Recomended document with IDs  [[1 3]]\n",
      "hiving distance  [[1.26626032 1.36748507]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(\"D\"+str(i)+\":\"+docs[i])\n",
    "test=input(\"Enter your text:\")\n",
    "docs.append(test+\":\")\n",
    "\n",
    "\n",
    "## Seperating the document into the label,striping off the unwanted white space\n",
    "x,y=[],[]\n",
    "for d in docs:\n",
    "    x.append(d[:d.index(\":\")].strip())\n",
    "    y.append(d[d.index(\":\")+1:].strip())\n",
    "    \n",
    "# vectorizer using Tfidf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer()\n",
    "vec=vectorizer.fit_transform(x)\n",
    "\n",
    "# trainning KNN Classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(1)\n",
    "knn.fit(vec[:6],y[:6])\n",
    "print(\"Label: \",knn.predict(vec[6]))\n",
    "\n",
    "# Sntiment Analysis\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "test_tokens=test.split(\" \")\n",
    "good=wordnet.synsets(\"good\")\n",
    "bad=wordnet.synsets(\"evil\")\n",
    "score_pos=0\n",
    "score_neg=0\n",
    "\n",
    "\n",
    "for token in test_tokens:\n",
    "    t=wordnet.synsets(token)\n",
    "    if len(t)>0:\n",
    "        sim_good=wordnet.wup_similarity(good[0],t[0])\n",
    "        sim_bad=wordnet.wup_similarity(bad[0],t[0])\n",
    "        if(sim_good is not None):\n",
    "            score_pos =score_pos + sim_good\n",
    "        if(sim_bad is not None):\n",
    "            score_neg =score_neg + sim_bad\n",
    "            \n",
    "            \n",
    "if((score_pos - score_neg)>0.1):\n",
    "    print(\"Subjective Statement, Positive openion of strength: %.2f\" %score_pos)\n",
    "    \n",
    "elif((score_neg - score_pos)>0.1):\n",
    "    print(\"Subjective Statement, Negative openion of strength: %.2f\" %score_neg)\n",
    "else:\n",
    "    print(\"Objective Statement, No openion Showed\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Nearest Document\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nb=NearestNeighbors(n_neighbors=2)\n",
    "nb.fit(vec[:6])\n",
    "closest_docs=nb.kneighbors(vec[6])\n",
    "print(\"Recomended document with IDs \",closest_docs[1])\n",
    "print(\"hiving distance \",closest_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
